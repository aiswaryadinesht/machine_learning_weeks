{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60e6a429-8727-4e21-8fef-25944f2977eb",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41afa26e-77ea-40b6-9fa8-c8cd12e8a58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['Natural', 'Language', 'Processing', 'is', 'fun', '!', 'Let', \"'s\", 'learn', 'it', 'together', '.']\n",
      "Sentence Tokens: ['Natural Language Processing is fun!', \"Let's learn it together.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = \"Natural Language Processing is fun! Let's learn it together.\"\n",
    "# Word Tokenization\n",
    "word_tokens = word_tokenize(text)\n",
    "print(\"Word Tokens:\", word_tokens)\n",
    "\n",
    "# Sentence Tokenization\n",
    "sentence_tokens = sent_tokenize(text)\n",
    "print(\"Sentence Tokens:\", sentence_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9592f7bf-a701-4d22-9b30-c611497e01bd",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd08167d-dbe4-4524-90d0-2b16386aa809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words: ['run', 'runner', 'ran', 'easili', 'fairli']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"running\", \"runner\", \"ran\", \"easily\", \"fairly\"]\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(\"Stemmed Words:\", stemmed_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b19758-fef0-400b-9873-d56cf0f6fb8b",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62522b46-b5db-4bfd-867b-d741fdbfa863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words: ['run', 'runner', 'run', 'easily', 'fairly']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [\"running\", \"runner\", \"ran\", \"easily\", \"fairly\"]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, pos=\"v\") for word in words]\n",
    "\n",
    "print(\"Lemmatized Words:\", lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a340186d-c3d4-4c5e-b2d7-c5025ca4adc2",
   "metadata": {},
   "source": [
    "# Name Entities Recognization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53e192d1-9882-489b-9cb9-ff0854fceb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple: ORG\n",
      "UK: GPE\n",
      "$1 billion: MONEY\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"Apple is looking at buying a UK-based startup for $1 billion.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text}: {ent.label_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2720bed-0ab3-4e6c-a5f6-e8f1fc12b9fd",
   "metadata": {},
   "source": [
    "# Ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ac04aa2-f716-4e29-a5f5-09a6d2a8ce31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'love'), ('love', 'natural'), ('natural', 'language'), ('language', 'processing')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Example text\n",
    "text = \"I love natural language processing\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = text.split()\n",
    "\n",
    "# Generate bigrams (n=2)\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "\n",
    "print(bigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3335815f-2d9e-413e-a5ae-f23cc7a378df",
   "metadata": {},
   "source": [
    "# Parts-of-Speech tagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02eea2d9-63b7-4645-8319-aac35107e85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "I love natural language processing\n",
      "\n",
      "PoS Tagging Result:\n",
      "running: VBG\n",
      "runner: NN\n",
      "ran: VBD\n",
      "easily: RB\n",
      "fairly: RB\n"
     ]
    }
   ],
   "source": [
    "# Importing the NLTK library\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Sample text\n",
    "#text = \"NLTK is a powerful library for natural language processing.\"\n",
    "\n",
    "# Performing PoS tagging\n",
    "pos_tags = pos_tag(words)\n",
    "\n",
    "# Displaying the PoS tagged result in separate lines\n",
    "print(\"Original Text:\")\n",
    "print(text)\n",
    "\n",
    "print(\"\\nPoS Tagging Result:\")\n",
    "for word, pos_tag in pos_tags:\n",
    "\tprint(f\"{word}: {pos_tag}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfb41768-dd10-4731-907f-781ad6b87f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:  SpaCy is a popular natural language processing library.\n",
      "PoS Tagging Result:\n",
      "SpaCy: PROPN\n",
      "is: AUX\n",
      "a: DET\n",
      "popular: ADJ\n",
      "natural: ADJ\n",
      "language: NOUN\n",
      "processing: NOUN\n",
      "library: NOUN\n",
      ".: PUNCT\n"
     ]
    }
   ],
   "source": [
    "#importing libraries \n",
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"SpaCy is a popular natural language processing library.\"\n",
    "\n",
    "# Process the text with SpaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Display the PoS tagged result\n",
    "print(\"Original Text: \", text)\n",
    "print(\"PoS Tagging Result:\")\n",
    "for token in doc:\n",
    "\tprint(f\"{token.text}: {token.pos_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854d415f-1c54-4f57-9774-9c61154c4e3d",
   "metadata": {},
   "source": [
    "# Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21012e4b-171e-411d-b95e-8c717093a2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: There is a pen on the table\n",
      "Text after Stopword Removal: pen table\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"There is a pen on the table\"\n",
    "\n",
    "# Process the text using spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Remove stopwords\n",
    "filtered_words = [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "# Join the filtered words to form a clean text\n",
    "clean_text = ' '.join(filtered_words)\n",
    "\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Text after Stopword Removal:\", clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd5d1b4f-a781-41de-8acb-10189fac17d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peter:PERSON\n",
      "anna:PERSON\n",
      "london:GPE\n",
      "Clean text: pen table\n",
      "peter: NN\n",
      "is: VBZ\n",
      "better: RBR\n",
      "student: NN\n",
      "and: CC\n",
      "runner: NN\n",
      ".: .\n",
      "he: PRP\n",
      "loves: VBZ\n",
      "to: TO\n",
      "play: VB\n",
      "games: NNS\n",
      ".: .\n",
      "everyone: NN\n",
      "likes: VBZ\n",
      "his: PRP$\n",
      "charcter.he: NN\n",
      "is: VBZ\n",
      "a: DT\n",
      "good: JJ\n",
      "boy: NN\n",
      ".: .\n",
      "anna: NN\n",
      "is: VBZ\n",
      "his: PRP$\n",
      "friend: NN\n",
      "who: WP\n",
      "lives: VBZ\n",
      "in: IN\n",
      "london: NN\n",
      "Stemmed Words: ['peter', 'is', 'better', 'student', 'and', 'runner', '.', 'he', 'love', 'to', 'play', 'game', '.', 'everyon', 'like', 'hi', 'charcter.h', 'is', 'a', 'good', 'boy', '.', 'anna', 'is', 'hi', 'friend', 'who', 'live', 'in', 'london']\n",
      "Lemmatized Words: ['peter', 'be', 'better', 'student', 'and', 'runner', '.', 'he', 'love', 'to', 'play', 'game', '.', 'everyone', 'like', 'his', 'charcter.he', 'be', 'a', 'good', 'boy', '.', 'anna', 'be', 'his', 'friend', 'who', 'live', 'in', 'london']\n",
      "-------------------------------------------------\n",
      "bigrams [('peter', 'is'), ('is', 'better'), ('better', 'student'), ('student', 'and'), ('and', 'runner'), ('runner', '.'), ('.', 'he'), ('he', 'loves'), ('loves', 'to'), ('to', 'play'), ('play', 'games'), ('games', '.'), ('.', 'everyone'), ('everyone', 'likes'), ('likes', 'his'), ('his', 'charcter.he'), ('charcter.he', 'is'), ('is', 'a'), ('a', 'good'), ('good', 'boy'), ('boy', '.'), ('.', 'anna'), ('anna', 'is'), ('is', 'his'), ('his', 'friend'), ('friend', 'who'), ('who', 'lives'), ('lives', 'in'), ('in', 'london')]\n",
      "Vectorization using Countvectorizer\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]]\n",
      "Vectorization using TfidfVectorizer\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         1.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         1.         0.         0.        ]\n",
      " [1.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         1.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  1.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         1.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  1.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         1.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         1.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.74639949\n",
      "  0.         0.         0.         0.         0.66549816 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         1.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         1.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         1.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         1.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         1.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         1.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  1.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         1.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import spacy\n",
    "\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "\n",
    "text = \"Peter is better student and runner. He loves to play games. Everyone likes his charcter.He is a good boy. Anna is his friend who lives in London\"\n",
    "\n",
    "text=text.lower()\n",
    "\n",
    "word_token=word_tokenize(text)\n",
    "sent_token=word_tokenize(text)\n",
    "stemmer=PorterStemmer()\n",
    "stemmed_word=[stemmer.stem(word)for word in word_token]\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "lemmatized_word=[lemmatizer.lemmatize(word,pos='v')for word in word_token]\n",
    "\n",
    "\n",
    "ner=nlp(text)\n",
    "for ent in ner.ents:\n",
    "    print(f\"{ent.text}:{ent.label_}\")\n",
    "\n",
    "filter_word=[token.text for token in doc if not token.is_stop]\n",
    "clean_text=\" \".join(filter_word)\n",
    "print(\"Clean text:\",clean_text)\n",
    "\n",
    "bigrams=list(ngrams(word_token,2))\n",
    "\n",
    "pos_tags=pos_tag(word_token)\n",
    "\n",
    "for word,pos_tag in pos_tags:\n",
    "    print(f\"{word}: {pos_tag}\")\n",
    "\n",
    "print(\"Stemmed Words:\", stemmed_word)\n",
    "print(\"Lemmatized Words:\", lemmatized_word)\n",
    "\n",
    "print(\"-------------------------------------------------\")\n",
    "print(\"bigrams\",bigrams)\n",
    "\n",
    "vectorizer1=CountVectorizer()\n",
    "vect_matrix=vectorizer1.fit_transform(lemmatized_word)\n",
    "print(\"Vectorization using Countvectorizer\")\n",
    "print(vect_matrix.toarray())\n",
    "\n",
    "vectorizer2=TfidfVectorizer()\n",
    "tf_matrix=vectorizer2.fit_transform(lemmatized_word)\n",
    "print(\"Vectorization using TfidfVectorizer\")\n",
    "print(tf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303c6ffd-321f-4daf-b442-811feb20fc63",
   "metadata": {},
   "source": [
    "# Vectorization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b96ae32-fd40-4212-ae10-23bb930c9d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'love': 3, 'programming': 5, 'in': 1, 'python': 6, 'is': 2, 'fun': 0, 'solving': 7, 'problems': 4, 'using': 8}\n",
      "BoW Matrix:\n",
      " [[0 1 0 1 0 1 1 0 0]\n",
      " [1 0 1 0 0 1 1 0 0]\n",
      " [0 0 0 1 1 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample data\n",
    "documents = [\n",
    "    \"I love programming in Python\",\n",
    "    \"Python programming is fun\",\n",
    "    \"I love solving problems using Python\"\n",
    "]\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "bow_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Display the vocabulary and the vectorized representation\n",
    "print(\"Vocabulary:\", vectorizer.vocabulary_)\n",
    "\n",
    "print(\"BoW Matrix:\\n\", bow_matrix.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e4e30f7-bc6f-4b9a-892f-7433a77bd502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.63174505 0.         0.4804584  0.         0.4804584\n",
      "  0.37311881 0.         0.        ]\n",
      " [0.5844829  0.         0.5844829  0.         0.         0.44451431\n",
      "  0.34520502 0.         0.        ]\n",
      " [0.         0.         0.         0.38376993 0.50461134 0.\n",
      "  0.29803159 0.50461134 0.50461134]]\n",
      "{'love': 3, 'programming': 5, 'in': 1, 'python': 6, 'is': 2, 'fun': 0, 'solving': 7, 'problems': 4, 'using': 8}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer2=TfidfVectorizer()\n",
    "\n",
    "tf_matrix=vectorizer2.fit_transform(documents)\n",
    "\n",
    "print(tf_matrix.toarray())\n",
    "print(vectorizer2.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e384d154-20d2-46b3-8bbe-9a74169fba7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a pen on the table\n"
     ]
    }
   ],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2db9ace-796e-48f0-a36e-b1ebce320f89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
