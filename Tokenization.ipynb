{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27c11ae6-ebfa-4c85-b829-c65328e0cd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['I', 'love', 'Python', '.', 'It', \"'\", 's', 'a', 'powerful', 'programming', 'language', '!']\n"
     ]
    }
   ],
   "source": [
    "# Word Tokenization from Scratch\n",
    "def word_tokenize(text):\n",
    "    tokens = []\n",
    "    word = \"\"\n",
    "    for char in text:\n",
    "        if char.isalnum(): # Check if character is alphanumeric (part of a word)\n",
    "            word += char\n",
    "            #print(word)\n",
    "        else:\n",
    "            if word:  # If we have collected a word, add it to tokens\n",
    "                tokens.append(word)\n",
    "                word = \"\"\n",
    "            if char.strip():  # Add punctuation or special characters as tokens\n",
    "                tokens.append(char)\n",
    "    if word:  # Append the last word if there's any\n",
    "        tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "# Input Text\n",
    "text = \"I love Python. It's a powerful programming language!\"\n",
    "\n",
    "# Tokenize into words\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Word Tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41509311-ce36-435c-8dea-38bc8fa1b01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokens: ['I love!', 'Python.', \"It's a powerful programming language!\", 'Do you like coding?']\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenization from Scratch\n",
    "def sentence_tokenize(text):\n",
    "    sentences = []\n",
    "    sentence = \"\"\n",
    "    for char in text:\n",
    "        sentence += char\n",
    "        if char in \".!?\":  # End of a sentence\n",
    "            sentences.append(sentence.strip())\n",
    "            sentence = \"\"\n",
    "    if sentence:  # Append the last sentence if there's any\n",
    "        sentences.append(sentence.strip())\n",
    "    return sentences\n",
    "\n",
    "# Input Text\n",
    "text = \"I love! Python. It's a powerful programming language! Do you like coding?\"\n",
    "\n",
    "# Tokenize into sentences\n",
    "sentences = sentence_tokenize(text)\n",
    "print(\"Sentence Tokens:\", sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d389cfb9-7b39-46cf-a738-2d0781441b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Tokens: ['P', 'y', 't', 'h', 'o', 'n']\n"
     ]
    }
   ],
   "source": [
    "# Character Tokenization from Scratch\n",
    "def character_tokenize(text):\n",
    "    return [char for char in text]  # Split into characters\n",
    "\n",
    "# Input Text\n",
    "text = \"Python\"\n",
    "\n",
    "# Tokenize into characters\n",
    "characters = character_tokenize(text)\n",
    "print(\"Character Tokens:\", characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e2a607c-eb60-4f0e-9d6a-2626ff0a41f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [1 1 1]\n",
      " [2 0 1]]\n",
      "['bat' 'cat' 'dog']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\"dog cat dog\", \"cat dog bat\", \"dog bat bat\"]\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert the sparse matrix to an array\n",
    "print(X.toarray())\n",
    "\n",
    "# Show the vocabulary\n",
    "print(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a057ef0d-e421-42bf-bb25-c26341e13b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Natural', 'Language', 'Processing', 'is', 'a', 'field', 'of', 'artificial', 'intelligence.'], ['Text', 'classification', 'and', 'sentiment', 'analysis', 'are', 'popular', 'NLP', 'tasks.'], ['Word', 'embeddings', 'are', 'used', 'to', 'represent', 'words', 'in', 'a', 'continuous', 'vector', 'space.']]\n"
     ]
    }
   ],
   "source": [
    "# Example of creating a simple corpus\n",
    "corpus = [\n",
    "    \"Natural Language Processing is a field of artificial intelligence.\",\n",
    "    \"Text classification and sentiment analysis are popular NLP tasks.\",\n",
    "    \"Word embeddings are used to represent words in a continuous vector space.\"\n",
    "]\n",
    "\n",
    "# Tokenize the corpus\n",
    "tokens = [sentence.split() for sentence in corpus]\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "759f5728-1db9-4c5f-9c36-7dccbf583cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'runner', 'easili', 'faster']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize the PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Example words\n",
    "words = [\"running\", \"runner\", \"easily\", \"faster\"]\n",
    "\n",
    "# Apply stemming\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(stemmed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b78b2ca-6540-45fc-bdd4-de89955ae8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'better', 'cat']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Example words\n",
    "words = [\"running\", \"better\", \"cats\"]\n",
    "\n",
    "# Apply lemmatization\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, pos=wordnet.VERB) for word in words]\n",
    "print(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c87deeb7-bb41-4196-877a-266325c0373c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple - ORG\n",
      "UK - GPE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the pre-trained model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example text\n",
    "text = \"Apple is looking to buy a startup in the UK.\"\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract named entities\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} - {ent.label_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31e81c05-c516-4df4-adef-c7c3f50990d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'love'), ('love', 'natural'), ('natural', 'language'), ('language', 'processing')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Example text\n",
    "text = \"I love natural language processing\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = text.split()\n",
    "\n",
    "# Generate bigrams (n=2)\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "\n",
    "print(bigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a656b322-9f40-46da-80aa-ecfbd0a6bbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['natural', 'language', 'processing', 'is', 'amazing'], ['text', 'classification', 'is', 'part', 'of', 'nlp'], ['word', 'embeddings', 'are', 'useful', 'in', 'nlp', 'tasks']]\n"
     ]
    }
   ],
   "source": [
    "# Example corpus\n",
    "corpus = [\n",
    "    \"Natural Language Processing is amazing.\",\n",
    "    \"Text classification is part of NLP.\",\n",
    "    \"Word embeddings are useful in NLP tasks.\"\n",
    "]\n",
    "\n",
    "# Tokenization from scratch\n",
    "def tokenize_corpus(corpus):\n",
    "    tokenized = []\n",
    "    for sentence in corpus:\n",
    "        # Convert to lowercase, split by whitespace, and remove punctuation\n",
    "        tokens = sentence.lower().replace('.', '').replace(',', '').split()\n",
    "        tokenized.append(tokens)\n",
    "    return tokenized\n",
    "\n",
    "# Tokenized corpus\n",
    "tokenized_corpus = tokenize_corpus(corpus)\n",
    "print(tokenized_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdbd3dc8-667b-4434-9b80-db0d826c7e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['runn', 'play', 'quick', 'cat', 'watch']\n"
     ]
    }
   ],
   "source": [
    "# Basic stemmer\n",
    "def simple_stemmer(word):\n",
    "    suffixes = [\"ing\", \"ed\", \"ly\", \"es\", \"s\"]\n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    return word\n",
    "\n",
    "# Example words\n",
    "words = [\"running\", \"played\", \"quickly\", \"cats\", \"watches\"]\n",
    "\n",
    "# Apply stemming\n",
    "stemmed_words = [simple_stemmer(word) for word in words]\n",
    "print(stemmed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0364519-db62-445a-bff0-d5d19c43f5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'good', 'cat', 'play']\n"
     ]
    }
   ],
   "source": [
    "# Simple lemmatizer with a dictionary of base forms\n",
    "def simple_lemmatizer(word):  \n",
    "    lemma_dict = {\n",
    "        \"running\": \"run\",\n",
    "        \"better\": \"good\",\n",
    "        \"cats\": \"cat\",\n",
    "        \"played\": \"play\",\n",
    "        \"faster\": \"fast\"\n",
    "    }\n",
    "    return lemma_dict.get(word, word)  # Return the word itself if no lemma found\n",
    "\n",
    "# Example words\n",
    "words = [\"running\", \"better\", \"cats\", \"played\"]\n",
    "\n",
    "# Apply lemmatization\n",
    "lemmatized_words = [simple_lemmatizer(word) for word in words]\n",
    "print(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a0ca1ce-863c-4df5-9e73-5d284112d9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams: [('I', 'love'), ('love', 'natural'), ('natural', 'language'), ('language', 'processing'), ('processing', 'and'), ('and', 'machine'), ('machine', 'learning')]\n",
      "Trigrams: [('I', 'love', 'natural'), ('love', 'natural', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', 'and'), ('processing', 'and', 'machine'), ('and', 'machine', 'learning')]\n",
      "unigrams: [('I',), ('love',), ('natural',), ('language',), ('processing',), ('and',), ('machine',), ('learning',)]\n"
     ]
    }
   ],
   "source": [
    "# Function to generate n-grams\n",
    "def generate_ngrams(tokens, n):\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngrams.append(tuple(tokens[i:i + n]))\n",
    "    return ngrams\n",
    "\n",
    "# Example text\n",
    "text = \"I love natural language processing and machine learning\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = text.split()\n",
    "\n",
    "# Generate bigrams (n=2)\n",
    "bigrams = generate_ngrams(tokens, 2)\n",
    "print(\"Bigrams:\", bigrams)\n",
    "\n",
    "# Generate trigrams (n=3)\n",
    "trigrams = generate_ngrams(tokens, 3)\n",
    "print(\"Trigrams:\", trigrams)\n",
    "\n",
    "bigrams = generate_ngrams(tokens, 1)\n",
    "print(\"unigrams:\", bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77455f4c-2453-4fd8-b509-dac34b131b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hai!', 'freiends,', '3', 'how', 'are', 'you?']\n",
      "['Hai', '!', 'freiends', ',', '3', 'how', 'are', 'you', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hai! freiends, 3 how are you?\"\n",
    "print(text.split())\n",
    "res = []\n",
    "tokenn = \"\"\n",
    "\n",
    "for words in text.split():\n",
    "    for char in words:    \n",
    "        if char.isalnum():  # Check if the character is alphanumeric\n",
    "            tokenn += char\n",
    "        else:  # If not alphanumeric, handle the token and special character\n",
    "            if tokenn:  # Add the current token if it's not empty\n",
    "                res.append(tokenn)\n",
    "                tokenn = \"\"  # Reset the token\n",
    "            res.append(char)  # Add the non-alphanumeric character as a token\n",
    "    if tokenn:  # If there's still a token after processing the word\n",
    "        res.append(tokenn)\n",
    "        tokenn = \"\"  # Reset the token for the next word\n",
    "\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448dd403-6bb5-424c-a498-5e0c7ee1be7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
